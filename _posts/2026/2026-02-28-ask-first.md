---
layout: post
title: "Ask First"
tags: adtech
---

*Written with Claude Opus 4.6 via [Claude Code](https://claude.ai/claude-code). I directed the argument and framing; Claude researched prior art and drafted prose.*

*Part of the [adtech](/adtech) series.*

---

## The Survival Problem

Every chatbot company is losing money on free users. OpenAI's inference bill is projected at [$14 billion for 2026](https://finance.yahoo.com/news/openais-own-forecast-predicts-14-150445813.html). Subscriptions have a ceiling — most people won't pay $20/month for a chatbot. That leaves three options:

1. **Sell to government.** Palantir-style contracts. Stable revenue, narrow customer base, political entanglement. You stop being a consumer product.
2. **Hope on VC optimism.** Raise another round. Burn it. Raise again. This works until the market corrects.
3. **Find an actual business model.** One that scales with usage, not with fundraising.

Advertising is the only business model that's ever scaled with free usage. Every major internet platform that serves free users runs on ads. The question isn't whether chatbot companies will run ads. It's how.

## How Not to Do It

ChatGPT rolled out ads on February 9. $60 CPM. $200K minimum buy. Ads appended to responses for free-tier users. The backlash was immediate: 68% negative sentiment on Reddit, a [QuitGPT](https://www.machine.news/chatgpt-ads-quitgpt/) boycott claiming 700,000 participants, Anthropic running a Super Bowl ad mocking the decision.

The format was polite — boxed, labeled "Sponsored," visually separated from the answer. Didn't matter. Search ads blend with content because the content is a list of links. Chat ads can't blend because the content is a direct answer to a personal question.

Perplexity [tried ads and killed them](/perplexity-was-right-to-kill-ads). Total revenue: $20,000. Their CEO's reasoning: *a user needs to believe this is the best possible answer.* But Perplexity still has the survival problem. Killing ads is a deferral, not a model. Anthropic's "no ads" positioning doesn't pay for inference either.

OpenAI's forced ads generated backlash, but [83% of free-tier users said they'd keep using ChatGPT anyway](https://www.forrester.com/blogs/what-consumers-actually-think-about-ads-in-chatgpt/). It works. It's just ugly.

## The Inversion

What if the user creates the ad inventory?

Not by tolerating an interruption, but by initiating it. A button — visible, optional, clearly labeled — that the user taps when they want to see what's available. The chatbot doesn't insert an ad into the conversation. The user pulls one in.

## Prior Art

This mechanism exists at scale in mobile gaming. Rewarded video ads are opt-in: the user taps a button to watch a 15-30 second ad in exchange for in-game currency or extra lives.

- **78% of mobile gamers** voluntarily watch rewarded video for in-game benefits
- **Completion rates exceed 95%**, vs. 60-70% for forced interstitials
- Rewarded video CPMs reach **$19.63** on US iOS, vs. **$14.32** for interstitials — a **37% premium** for the opt-in format

These numbers come from gamers primed for this exchange. Chatbot users are different — the value proposition is less visceral than extra lives, and the context is more serious. The stats don't transfer directly. But the direction is clear: advertisers pay more for voluntary attention.

Brave Browser extended opt-in to web browsing. Users choose to see ads as push notifications, receive 70% of revenue as BAT tokens. 100 million monthly active users. $100M annualized revenue. But individual earnings are modest (~$0.60-$1.00/month), and advertiser adoption remains niche. Proof of concept, not proof of scale.

## Applied to Chat

The opt-in surface is a button in the interface margin. Not inside the conversation. Not appended to an answer.

When the user taps it, the chatbot sends the current conversation context — as an embedding, inside a [sealed TEE enclave](/perplexity-was-right-to-kill-ads) — to the exchange. The exchange returns relevant ads in a dedicated panel, separate from the conversation.

**High purchase intent.** The user is asking about running shoes for flat feet. They tap the button. The exchange returns ads from Brooks, ASICS, and a local running store. The advertiser gets a lead from someone who had a conversation, formed a preference, and then deliberately asked to see options.

**Low purchase intent.** The user is asking about the history of brutalist architecture. They ignore the button. No ad is shown. No attention is demanded. If curiosity leads them to tap anyway, they see an architecture bookstore, a concrete furniture brand, a walking tour. Low intent, but genuine attention.

## The Quality Signal

A forced impression tells the advertiser: this person was in the room. An opt-in impression tells the advertiser: this person raised their hand.

Exchanges already price intent aggressively. Google charges over $50/click for high commercial intent keywords like insurance quotes, and under $2/click for entertainment — a 25x spread driven by proximity to a purchase decision.

An opt-in impression in a conversational context adds a layer: the user had a conversation, and then took a deliberate action to see commercial options. That's not a replacement for the targeting signals search engines have built over decades. But it's a signal that doesn't exist anywhere else: confirmed willingness to engage with advertising at the moment of highest contextual relevance.

The exchange can price this as a premium tier — not because of a self-reported label, but because of what it produces.

## Pricing Without Trust

The exchange can't rely on a platform's word that an impression was user-initiated.

A platform could flag every impression as opt-in and collect the premium. The [TEE](/perplexity-was-right-to-kill-ads) attests the auction code. It doesn't attest the UI. The button tap happens in the platform's application layer, outside the enclave. A self-reported flag is worthless.

The exchange prices based on what it can observe: behavior, aggregated at platform scale.

A platform routing thousands of impressions per hour produces a behavioral fingerprint. Genuine opt-in inventory performs differently from forced inventory on every downstream metric: click-through rate, conversion rate, bounce rate, time-on-site. In mobile gaming, the gap between rewarded video (95% completion) and interstitials (60-70%) is visible in the first day of data. The exchange doesn't verify individual impressions. It scores platforms.

Display advertising already works this way. Exchanges verify with third-party measurement (IAS, MOAT, DoubleVerify) and reprice inventory that underperforms its declared tier. Chat inventory will need equivalent measurement infrastructure, which doesn't exist yet.

## Inference for Attention

The opt-in model can fund free-tier access directly. The user trades attention for compute. Watch an ad, earn inference credits. This is Brave's model applied to inference instead of browsing.

But "deliberate transaction" overstates the voluntariness. A user who depends on the chatbot and can't afford the subscription isn't making a free choice — they're making a constrained one. The same critique applies to every ad-supported free tier. It's better than forced ads because the user controls the timing. The power asymmetry is real. Attention is the price for those who can't pay cash.

## What Could Go Wrong

**The button changes the relationship.** Even untapped, its presence signals "this chatbot is an ad platform." This is Perplexity's CEO's argument — the *presence* of advertising undermines trust, regardless of format. A chatbot with an ad button is a different product than a chatbot without one. The question is whether the trust cost is lower than forced ads, and whether it's survivable.

**Incentive corruption.** If the chatbot earns revenue when users tap the button, it has an incentive to steer conversations toward purchase intent, or to make answers slightly less complete so users feel compelled to "explore options." The TEE attests the auction, not the answer generation. The conflict of interest doesn't disappear — it moves upstream from ad selection to answer quality.

**Context leakage.** When the user taps the button, the conversation embedding goes to the exchange via TEE. But the user may have shared sensitive information — health concerns, financial problems, relationship issues — that they'd tell a chatbot but not an ad targeting system. Opting in to see ads is not informed consent to have the full conversation scored for ad relevance.

**Low opt-in rates.** Mobile gaming rewarded video reaches 38% of daily active users, but extra lives are more immediate than inference credits. A chatbot might see far lower rates.

**Dark patterns.** Opt-in degrades to forced if the platform manufactures consent — misleading buttons, auto-expanding panels. The exchange can detect this statistically (dark-pattern "opt-in" produces metrics indistinguishable from forced impressions), but detection is retrospective.

**Curiosity fatigue.** The long-term opt-in rate in low-intent contexts may converge to zero, leaving only high-intent queries as viable inventory. That might be fine — high-intent is where the value is — but it limits the model's reach.

## The Least Bad Option

Opt-in advertising in chat is not clean. It creates incentive problems. It puts a price on conversations that users thought were private.

But the alternative isn't "no ads and everything is fine." The alternative is:

- **Forced ads**, which are worse on every dimension. OpenAI is proving this in real time.
- **Subscription only**, which caps your market at users willing to pay. VC patience has limits.
- **Government and enterprise contracts**, which turn a consumer product into a vendor relationship.
- **Death.** Most chatbot companies will not survive current burn rates.

Opt-in is the least bad version of the only model that scales. It preserves more user agency than forced ads. It generates higher-quality inventory for advertisers. It lets the [embedding-space exchange](/power-diagrams-ad-auctions) price intent by observed behavior rather than self-reported labels. It funds inference for users who can't pay.

The tradeoffs are real. But the trust chain has three links, and each one is solvable:

1. **Verifiable embeddings.** [Open-weight models](/the-convergence) with published hashes. Anyone can reproduce the embedding and verify the auction scored it correctly.
2. **Verifiable auction.** A [sealed TEE enclave](/perplexity-was-right-to-kill-ads) running attested code. The scoring function is published, audited, and cryptographically proven to be what executed.
3. **User-initiated impressions.** The user asks first. No ad appears without a deliberate action.

Each link removes a reason to distrust. What they don't solve is answer independence — whether the chatbot steers conversations toward purchase intent to generate more ad taps. No protocol solves that. Competition does. Switching costs between chatbots are near zero. A chatbot that degrades its answers to sell more ads loses users to one that doesn't. That's the same constraint that keeps every ad-supported medium roughly honest, and it requires no engineering.

The user asks first. That's not a perfect standard. It's the best one available.

---

*Part of the [adtech](/adtech) series. june@june.kim*
