---
layout: post
title: "Ask First"
tags: vector-space
image: "/assets/dots.png"
---

*Written with Claude Opus 4.6 via [Claude Code](https://claude.ai/claude-code). I directed the argument and framing; Claude researched prior art and drafted prose.*

*Part of the [Vector Space](/vector-space) series.*

---

Every chatbot company is losing money on free users. OpenAI's inference bill is projected at [$14 billion for 2026](https://finance.yahoo.com/news/openais-own-forecast-predicts-14-150445813.html). Subscriptions have a ceiling. Advertising is the only business model that's ever scaled with free usage. The question isn't whether chatbot companies will run ads. It's how — without destroying the trust that makes a chatbot worth using.

ChatGPT [rolled out ads](https://openai.com/index/testing-ads-in-chatgpt/) on February 9. Perplexity [tried and killed them](/perplexity-was-right-to-kill-ads). Both failed the same test: the moment a chatbot shows an ad it selected, the user wonders whether the answer is honest. The conflict of interest is structural, not cosmetic. No amount of labeling fixes it.

The fix is to separate the signal from the sale, and to separate both from the chatbot. But first, it helps to distinguish three things that all get called "ads."

## Three Kinds of Ad

**Ads you endure.** A pre-roll video before the thing you came for. A sponsored box in your ChatGPT answer. A billboard on the highway. The ad is a tax on your attention — the price of accessing something else. You tolerate it. You never asked for it. Every platform that forces these ads is betting that the content is worth more than the interruption costs. Sometimes it is. The resentment accumulates anyway.

**Ads disguised as experience.** An affiliate review that reads like editorial. A native ad styled as a feed post. A chatbot recommendation that's quietly sponsored. The commercial intent is hidden because revealing it would break the spell. These are more pleasant than interruptions — until you realize the advice wasn't honest. The trust damage is worse precisely because you didn't see it coming.

**Ads you'd miss if they were gone.** You open Google Maps and search "restaurants near me." The pins are ads. Nobody gets upset — removing them would make the product useless. Amazon search results are ads — and they're exactly what you came for. Yelp listings are ads — and 90% of users buy within a week. When your intent is already commercial, relevant advertising isn't a tax. It's a service. Remove it and the experience gets worse.

The first two categories are what people mean when they say they hate ads. The third is what people mean when they say "I don't mind ads if they're relevant" — but the relevance has to be real, and the intent has to be theirs.

The problem with chatbot advertising isn't that ads exist. It's that forced ads poison the conversation even when the user's intent is commercial, because the user can't tell whether the answer is honest. And disguised ads are worse — they poison the conversation *especially* when it seems honest. The two-phase model is designed for the third category: surface commercial information only when the user's conversation is already headed somewhere commercial, and only when the user asks.

![Five dots growing brighter, from nearly invisible to warm amber — proximity to expertise rendered as light.](/assets/dots.png)

## Two Phases

**Phase one: proximity.** A system that is not the chatbot monitors the conversation's embedding — the vector representation of what's being discussed. As the conversation moves through embedding space, a small indicator in the UI reflects how close the nearest advertiser is. A dot that brightens. A line that extends. Something peripheral — readable at a glance, ignorable by default.

The indicator maps directly to cosine distance. No interpretation layer, no algorithm deciding "is this relevant enough to show." Just: how close is the nearest expertise to where this conversation currently is? The answer is a number. The number is a brightness. If no advertiser has positioned anywhere nearby, the indicator is dark. There's nothing to see.

This indicator is not generated by the chatbot. The chatbot doesn't know it exists. It's produced by a separate system that reads the conversation's meaning but cannot write to the conversation. No auction has run. No advertiser has been selected. No money has moved. The user sees proximity to a region of expertise, not a business name.

**Phase two: auction.** The user taps the indicator. Now the full auction fires inside a [TEE enclave](/perplexity-was-right-to-kill-ads): `score = log(bid) - distance² / σ²`. Winner selected, second-price payment, result presented. The advertiser pays only for impressions the user asked for.

Phase one is a passive signal — continuous, ambient, no commercial transaction. Phase two is the market — competitive, priced, consented to. The user decides when to cross the boundary. There's no threshold for the system to game because there's no threshold — just a distance rendered as a brightness, and a person who chooses when to act on it.

The order matters. Traditional advertising: pay money, get shown, hope for relevance. Permission marketing: ask for permission, then try to be relevant. This model: prove relevance, then earn permission, then get shown. The advertiser must position accurately in embedding space — close to the problem they actually solve. The indicator reflecting that proximity is the proof rendered visually. The user tapping is permission granted on the basis of proven relevance. The auction is the sale, after both conditions are met. Relevance is not optimized after the impression. It's the prerequisite for one.

## Browsing the Ad Space

Each message in the conversation produces a new embedding, a new position in the space. The indicator updates continuously.

The user says "my basement floods every spring." The dot warms slightly — waterproofing contractors exist in this region. The user continues: "I think it's the sewer line." Brighter — sewer repair specialists are closer. The user asks: "what about French drains?" The brightness shifts. Each conversational turn moves the user through embedding space, and the indicator reflects what's nearby at each step.

The user doesn't have to notice. The natural conversation is the browse. But they *could* watch the indicator and deliberately explore — asking about alternatives, narrowing their problem, expanding their options — following the signal toward denser regions of expertise. Window shopping in embedding space. No store is entered until the user taps. No auction fires until they ask.

Seth Godin argued in [*Permission Marketing*](https://en.wikipedia.org/wiki/Permission_marketing) (1999) that advertising should be anticipated, personal, and relevant — and that marketers should seek permission before communicating. Doc Searls extended this in [*The Intention Economy*](https://cyber.harvard.edu/projectvrm/Intentcasting) (2012): buyers broadcast intent to the marketplace, and the market responds. For twenty years, every attempt to build this failed — [Yellcast, Intently, Budggy, Ubokia](https://cyber.harvard.edu/projectvrm/VRM_Development_Work) — because intentcasting required users to fill out forms, structure their needs, manage responses. The friction was fatal. Google captured intent implicitly through search queries, with less work.

Nobody implemented permission marketing at the per-impression level. The two-phase model does. Conversational browsing is zero-friction intentcasting. You don't declare intent. You just talk. Each message casts your intent into the embedding space, and the market responds with what's nearby. The twenty-year failure wasn't a failure of the idea. It was a failure of the interface.

## The UX Is the Trust

The architecture means nothing if the experience doesn't feel right.

[Pinterest proved this](https://business.pinterest.com/blog/how-positive-platforms-impact-marketing-performance/). In a study with MAGNA testing 6,200 participants, the *same exact ad* shown on a positively-perceived platform was rated 2x as trustworthy, with 94% higher purchase intent. The ad didn't change. The context did.

[WeChat mini-programs](https://www.tmogroup.asia/insights/wechat-mini-program-touch-points/) are the strongest precedent. They process $100 billion in annual transactions with 945 million users, and they *architecturally cannot push messages* — they can only respond when users initiate. The constraint isn't decorative. It's the reason the platform works for commerce. Users trust it because the system can't interrupt them.

The data on voluntary commercial engagement is unambiguous. Amazon Sponsored Products (user already shopping) convert at [9.5%](https://www.sarasanalytics.com/blog/amazon-ads-conversion-rate) vs general ecommerce at 1.33% — a 7.5x gap. Google Search converts at [7-13x](https://www.storegrowers.com/google-ads-benchmarks/) the rate of Display. [Yelp users](https://searchengineland.com/survey-97-of-online-adults-transact-with-businesses-they-find-on-yelp-323881): 90% purchase within a week, 42% within 24 hours — because they arrived with declared intent. The pattern is consistent: when people enter a commercial context voluntarily because the information serves them, every downstream metric improves by an order of magnitude.

Don Marti [argued](https://blog.zgp.org/perfectly-targeted-advertising-would-be-perfectly-worthless/) that perfectly targeted surveillance ads are perfectly worthless — targeting eliminates the waste that signals a company's commitment and solvency. A retargeted ad says "this company knows you were on their website." An embedding-matched indicator in an opt-in context says "this company positioned themselves near your problem." The first carries zero signal. The second is a credible claim of expertise. The two-phase model restores the signaling value that surveillance targeting destroyed.

A chatbot where a faint signal reflects proximity to expertise, outside the conversation, leading somewhere only when the user taps — that's a context where commercial information feels like a resource, not an interruption.

The [architecture](/model-blindness) provides the guarantee — attested separation, one-directional data flow, auditable code. But the guarantee is not the trust. Trust is earned over time. The user taps the indicator, gets something useful, and the conversation continues honestly. They ignore it, and nothing changes. Over weeks and months of that consistency, the system earns what no whitepaper can claim. The architecture makes sure the consistency isn't a lie.

## What Could Go Wrong

**The indicator itself is commercial.** Even as a passive signal, its presence means "this chatbot has an ad system." A faint dot during a conversation about a medical diagnosis could feel wrong. The indicator's subtlety helps — it demands no attention — but its existence changes the product. The question is whether a passive proximity signal costs less trust than forced ads. Everything we know says yes, but the cost isn't zero.

**Context leakage.** When the user taps the indicator, the conversation embedding enters the auction enclave. The user may have shared sensitive information they'd tell a chatbot but not an ad system. The two-phase model limits this: phase one is a proximity check computed locally that reveals nothing to the exchange. Phase two sends the embedding into a sealed enclave that returns only a winner and a price. The exchange never sees the raw embedding. But "sealed" is only as trustworthy as the attestation infrastructure, and TEE side-channel attacks [exist](https://arxiv.org/abs/2006.13598).

**Low tap rates.** Many users will never tap. High-intent queries may be the only viable inventory. That might be fine — high-intent is where the value is — but it limits the model's reach.

**This is a design, not a deployment.** The two-phase mechanism has not been built. No one has shipped per-impression permission marketing in a chatbot. The closest analogue is [Google Ad Intents](https://support.google.com/adsense/answer/13844047) — contextual chips that delay ad loading until click — but the chips are styled as content navigation, not as a commercial consent gate. The individual components exist in production (TEE-attested auctions, embedding infrastructure, confidential computing). The integrated system does not.

**Competition is the real check.** The platform that deploys the chatbot still chooses the embedding model and designs the indicator's UX. A platform that makes the indicator too prominent loses users to one that doesn't. Switching costs between chatbots are near zero. That competitive pressure is the constraint that keeps the system honest — and it requires no engineering.

---

*Part of the [Vector Space](/vector-space) series. The architecture that guarantees the UX isn't lying is described in [Model Blindness](/model-blindness). june@june.kim*
